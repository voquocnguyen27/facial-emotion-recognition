{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8061616,"sourceType":"datasetVersion","datasetId":4755432},{"sourceId":8677929,"sourceType":"datasetVersion","datasetId":4803141},{"sourceId":8839345,"sourceType":"datasetVersion","datasetId":4755313},{"sourceId":8839347,"sourceType":"datasetVersion","datasetId":4755265}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T08:19:27.114979Z","iopub.execute_input":"2024-07-02T08:19:27.115729Z","iopub.status.idle":"2024-07-02T08:19:28.411931Z","shell.execute_reply.started":"2024-07-02T08:19:27.115695Z","shell.execute_reply":"2024-07-02T08:19:28.410942Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import shutil\nsrc_path = r\"/kaggle/input/configs/fer2013_configs.json\"\ndst_path = r\"/kaggle/working/fer2013_configs.json\"\nshutil.copy(src_path, dst_path)\nprint('Copied')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T08:19:28.413757Z","iopub.execute_input":"2024-07-02T08:19:28.414527Z","iopub.status.idle":"2024-07-02T08:19:29.255250Z","shell.execute_reply.started":"2024-07-02T08:19:28.414487Z","shell.execute_reply":"2024-07-02T08:19:29.254392Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Copied\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r /kaggle/input/configs/requirements.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Định nghĩa hàm tạo một lớp tích chập 2 chiều với bộ lọc 3x3\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n# Định nghĩa hàm tạo một lớp tích chập 2 chiều với bộ lọc 1x1\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n# Định nghĩa một Block\nclass BasicBlock(nn.Module):\n    expansion = 1\n    __constants__ = [\"downsample\"]\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        groups=1,\n        base_width=64,\n        dilation=1,\n        norm_layer=None,\n    ):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = [\"downsample\"]\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        groups=1,\n        base_width=64,\n        dilation=1,\n        norm_layer=None,\n    ):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers, # Số layer của block\n        num_classes=1000,\n        zero_init_residual=False,\n        groups=1,\n        width_per_group=64,\n        replace_stride_with_dilation=None,\n        norm_layer=None,\n        in_channels=3,\n    ):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n\n        # NOTE: strictly set the in_channels = 3 to load the pretrained model\n        self.conv1 = nn.Conv2d(\n            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n        )\n        # self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(\n            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n        )\n        self.layer3 = self._make_layer(\n            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n        )\n        self.layer4 = self._make_layer(\n            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # NOTE: strictly set the num_classes = 1000 to load the pretrained model\n        self.fc = nn.Linear(512 * block.expansion, 1000)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride,\n                downsample,\n                self.groups,\n                self.base_width,\n                previous_dilation,\n                norm_layer,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    dilation=self.dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:39:48.876366Z","iopub.execute_input":"2024-06-10T15:39:48.876765Z","iopub.status.idle":"2024-06-10T15:39:48.913689Z","shell.execute_reply.started":"2024-06-10T15:39:48.876704Z","shell.execute_reply":"2024-06-10T15:39:48.912785Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import traceback\n\ndef up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n    return nn.Sequential(\n        nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=kernel_size, stride=stride\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass Masking4(nn.Module):\n    def __init__(self, in_channels, out_channels, block=BasicBlock):\n        assert in_channels == out_channels\n        super(Masking4, self).__init__()\n        filters = [\n            in_channels,\n            in_channels * 2,\n            in_channels * 4,\n            in_channels * 8,\n            in_channels * 16,\n        ]\n\n        self.downsample1 = nn.Sequential(\n            conv1x1(filters[0], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample2 = nn.Sequential(\n            conv1x1(filters[1], filters[2], 1),\n            nn.BatchNorm2d(filters[2]),\n        )\n\n        self.downsample3 = nn.Sequential(\n            conv1x1(filters[2], filters[3], 1),\n            nn.BatchNorm2d(filters[3]),\n        )\n\n        self.downsample4 = nn.Sequential(\n            conv1x1(filters[3], filters[4], 1),\n            nn.BatchNorm2d(filters[4]),\n        )\n\n        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n        self.conv4 = block(filters[3], filters[4], downsample=self.downsample4)\n\n        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n\n        self.downsample5 = nn.Sequential(\n            conv1x1(filters[4], filters[3], 1),\n            nn.BatchNorm2d(filters[3]),\n        )\n\n        self.downsample6 = nn.Sequential(\n            conv1x1(filters[3], filters[2], 1),\n            nn.BatchNorm2d(filters[2]),\n        )\n\n        self.downsample7 = nn.Sequential(\n            conv1x1(filters[2], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample8 = nn.Sequential(\n            conv1x1(filters[1], filters[0], 1),\n            nn.BatchNorm2d(filters[0]),\n        )\n\n        self.up_pool5 = up_pooling(filters[4], filters[3])\n        self.conv5 = block(filters[4], filters[3], downsample=self.downsample5)\n        self.up_pool6 = up_pooling(filters[3], filters[2])\n        self.conv6 = block(filters[3], filters[2], downsample=self.downsample6)\n        self.up_pool7 = up_pooling(filters[2], filters[1])\n        self.conv7 = block(filters[2], filters[1], downsample=self.downsample7)\n        self.conv8 = block(filters[1], filters[0], downsample=self.downsample8)\n\n        # init weight\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        for m in self.modules():\n            if isinstance(m, Bottleneck):\n                nn.init.constant_(m.bn3.weight, 0)\n            elif isinstance(m, BasicBlock):\n                nn.init.constant_(m.bn2.weight, 0)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        p1 = self.down_pooling(x1)\n        x2 = self.conv2(p1)\n        p2 = self.down_pooling(x2)\n        x3 = self.conv3(p2)\n        p3 = self.down_pooling(x3)\n        x4 = self.conv4(p3)\n\n        x5 = self.up_pool5(x4)\n        x5 = torch.cat([x5, x3], dim=1)\n        x5 = self.conv5(x5)\n\n        x6 = self.up_pool6(x5)\n        x6 = torch.cat([x6, x2], dim=1)\n        x6 = self.conv6(x6)\n\n        x7 = self.up_pool7(x6)\n        x7 = torch.cat([x7, x1], dim=1)\n        x7 = self.conv7(x7)\n\n        x8 = self.conv8(x7)\n\n        output = torch.softmax(x8, dim=1)\n        # output = torch.sigmoid(x8)\n        return output\n\n\nclass Masking3(nn.Module):\n    def __init__(self, in_channels, out_channels, block=BasicBlock):\n        assert in_channels == out_channels\n        super(Masking3, self).__init__()\n        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n\n        self.downsample1 = nn.Sequential(\n            conv1x1(filters[0], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample2 = nn.Sequential(\n            conv1x1(filters[1], filters[2], 1),\n            nn.BatchNorm2d(filters[2]),\n        )\n\n        self.downsample3 = nn.Sequential(\n            conv1x1(filters[2], filters[3], 1),\n            nn.BatchNorm2d(filters[3]),\n        )\n\n        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n\n        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n\n        self.downsample4 = nn.Sequential(\n            conv1x1(filters[3], filters[2], 1),\n            nn.BatchNorm2d(filters[2]),\n        )\n\n        self.downsample5 = nn.Sequential(\n            conv1x1(filters[2], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample6 = nn.Sequential(\n            conv1x1(filters[1], filters[0], 1),\n            nn.BatchNorm2d(filters[0]),\n        )\n\n        self.up_pool4 = up_pooling(filters[3], filters[2])\n        self.conv4 = block(filters[3], filters[2], downsample=self.downsample4)\n        self.up_pool5 = up_pooling(filters[2], filters[1])\n        self.conv5 = block(filters[2], filters[1], downsample=self.downsample5)\n\n        self.conv6 = block(filters[1], filters[0], downsample=self.downsample6)\n\n        # init weight\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        for m in self.modules():\n            if isinstance(m, Bottleneck):\n                nn.init.constant_(m.bn3.weight, 0)\n            elif isinstance(m, BasicBlock):\n                nn.init.constant_(m.bn2.weight, 0)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        p1 = self.down_pooling(x1)\n        x2 = self.conv2(p1)\n        p2 = self.down_pooling(x2)\n        x3 = self.conv3(p2)\n\n        x4 = self.up_pool4(x3)\n        x4 = torch.cat([x4, x2], dim=1)\n\n        x4 = self.conv4(x4)\n\n        x5 = self.up_pool5(x4)\n        x5 = torch.cat([x5, x1], dim=1)\n        x5 = self.conv5(x5)\n\n        x6 = self.conv6(x5)\n\n        output = torch.softmax(x6, dim=1)\n        # output = torch.sigmoid(x6)\n        return output\n\n\nclass Masking2(nn.Module):\n    def __init__(self, in_channels, out_channels, block=BasicBlock):\n        assert in_channels == out_channels\n        super(Masking2, self).__init__()\n        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n\n        self.downsample1 = nn.Sequential(\n            conv1x1(filters[0], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample2 = nn.Sequential(\n            conv1x1(filters[1], filters[2], 1),\n            nn.BatchNorm2d(filters[2]),\n        )\n\n        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n\n        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n\n        self.downsample3 = nn.Sequential(\n            conv1x1(filters[2], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.downsample4 = nn.Sequential(\n            conv1x1(filters[1], filters[0], 1),\n            nn.BatchNorm2d(filters[0]),\n        )\n\n        self.up_pool3 = up_pooling(filters[2], filters[1])\n        self.conv3 = block(filters[2], filters[1], downsample=self.downsample3)\n        self.conv4 = block(filters[1], filters[0], downsample=self.downsample4)\n\n        # init weight\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        for m in self.modules():\n            if isinstance(m, Bottleneck):\n                nn.init.constant_(m.bn3.weight, 0)\n            elif isinstance(m, BasicBlock):\n                nn.init.constant_(m.bn2.weight, 0)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        p1 = self.down_pooling(x1)\n        x2 = self.conv2(p1)\n\n        x3 = self.up_pool3(x2)\n        x3 = torch.cat([x3, x1], dim=1)\n        x3 = self.conv3(x3)\n\n        x4 = self.conv4(x3)\n\n        output = torch.softmax(x4, dim=1)\n        # output = torch.sigmoid(x4)\n        return output\n\n\nclass Masking1(nn.Module):\n    def __init__(self, in_channels, out_channels, block=BasicBlock):\n        assert in_channels == out_channels\n        super(Masking1, self).__init__()\n        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n\n        self.downsample1 = nn.Sequential(\n            conv1x1(filters[0], filters[1], 1),\n            nn.BatchNorm2d(filters[1]),\n        )\n\n        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n\n        self.downsample2 = nn.Sequential(\n            conv1x1(filters[1], filters[0], 1),\n            nn.BatchNorm2d(filters[0]),\n        )\n\n        self.conv2 = block(filters[1], filters[0], downsample=self.downsample2)\n\n        # init weight\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        for m in self.modules():\n            if isinstance(m, Bottleneck):\n                nn.init.constant_(m.bn3.weight, 0)\n            elif isinstance(m, BasicBlock):\n                nn.init.constant_(m.bn2.weight, 0)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.conv2(x1)\n        output = torch.softmax(x2, dim=1)\n        # output = torch.sigmoid(x2)\n        return output\n\n\ndef masking(in_channels, out_channels, depth, block=BasicBlock):\n    if depth == 1:\n        return Masking1(in_channels, out_channels, block)\n    elif depth == 2:\n        return Masking2(in_channels, out_channels, block)\n    elif depth == 3:\n        return Masking3(in_channels, out_channels, block)\n    elif depth == 4:\n        return Masking4(in_channels, out_channels, block)\n    else:\n        traceback.print_exc()\n        raise Exception(\"depth need to be from 0-3\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:12:58.107398Z","iopub.execute_input":"2024-06-10T15:12:58.107951Z","iopub.status.idle":"2024-06-10T15:12:58.166250Z","shell.execute_reply.started":"2024-06-10T15:12:58.107917Z","shell.execute_reply":"2024-06-10T15:12:58.165493Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ResMasking(ResNet):\n    def __init__(self, weight_path):\n        super(ResMasking, self).__init__(\n            block=BasicBlock, layers=[3, 4, 6, 3], in_channels=3, num_classes=1000\n        )\n\n        self.fc = nn.Linear(512, 7)\n\n        self.mask1 = masking(64, 64, depth=4)\n        self.mask2 = masking(128, 128, depth=3)\n        self.mask3 = masking(256, 256, depth=2)\n        self.mask4 = masking(512, 512, depth=1)\n\n    def forward(self, x):  # 224\n        x = self.conv1(x)  # 112\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)  # 56\n\n        x = self.layer1(x)  # 56\n        m = self.mask1(x)\n        x = x * (1 + m)\n        # x = x * m\n\n        x = self.layer2(x)  # 28\n        m = self.mask2(x)\n        x = x * (1 + m)\n        # x = x * m\n\n        x = self.layer3(x)  # 14\n        m = self.mask3(x)\n        x = x * (1 + m)\n        # x = x * m\n\n        x = self.layer4(x)  # 7\n        m = self.mask4(x)\n        x = x * (1 + m)\n        # x = x * m\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.fc(x)\n        return x\n\n\ndef resmasking(in_channels, num_classes, weight_path=\"\"):\n    return ResMasking(weight_path)\n\n\ndef resmasking_dropout1(in_channels=3, num_classes=7, weight_path=\"\"):\n    model = ResMasking(weight_path)\n    model.fc = nn.Sequential(\n        nn.Dropout(0.4),\n        nn.Linear(512, 7)\n        # nn.Linear(512, num_classes)\n    )\n    return model\n\n\ndef resmasking_dropout2(in_channels, num_classes, weight_path=\"\"):\n    model = ResMasking(weight_path)\n\n    model.fc = nn.Sequential(\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Dropout(p=0.5),\n        nn.Linear(128, 7),\n    )\n    return model\n\n\ndef resmasking_dropout3(in_channels, num_classes, weight_path=\"\"):\n    model = ResMasking(weight_path)\n\n    model.fc = nn.Sequential(\n        nn.Linear(512, 512),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(512, 128),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(128, 7),\n    )\n    return model\n\n\ndef resmasking_dropout4(in_channels, num_classes, weight_path=\"\"):\n    model = ResMasking(weight_path)\n\n    model.fc = nn.Sequential(\n        nn.Linear(512, 128),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(128, 128),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(128, 7),\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:37:12.684979Z","iopub.execute_input":"2024-06-10T15:37:12.685385Z","iopub.status.idle":"2024-06-10T15:37:12.702659Z","shell.execute_reply.started":"2024-06-10T15:37:12.685355Z","shell.execute_reply":"2024-06-10T15:37:12.701767Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\nfrom imgaug import augmenters as iaa\n\nEMOTION_DICT = {\n    0: \"angry\",\n    1: \"disgust\",\n    2: \"fear\",\n    3: \"happy\",\n    4: \"sad\",\n    5: \"surprise\",\n    6: \"neutral\",\n}\n\n\n'''\nBiến đổi làm giàu dữ liệu:\n- Lật ảnh theo trục dọc\n- Xoay ảnh từ -30 đến 30 độ\n'''\nseg = iaa.Sequential(\n    [\n        iaa.Fliplr(p=0.5),\n        iaa.Affine(rotate=(-30, 30)),\n    ]\n)\n\n\n'''\n- Lấy ảnh và nhãn gốc từ danh sách dữ liệu.\n- Sửa kích cỡ ảnh về 224×224.\n- Chồng ảnh lên 3 lần tạo ra 3 kênh.\n- Thực hiện làm giàu nếu như đang trong quá trình huấn luyện.\n- Chuyển ảnh về Tensor.\n'''\nclass FER2013(Dataset):\n    def __init__(self, stage, configs, tta=False, tta_size=48):\n        self._stage = stage\n        self._configs = configs\n        self._tta = tta\n        self._tta_size = tta_size\n\n        self._image_size = (configs[\"image_size\"], configs[\"image_size\"])\n\n        self._data = pd.read_csv(\n            os.path.join(configs[\"data_path\"], \"{}.csv\".format(stage))\n        )\n\n        self._pixels = self._data[\"pixels\"].tolist()\n        self._emotions = pd.get_dummies(self._data[\"emotion\"])\n\n        self._transform = transforms.Compose(\n            [\n                transforms.ToPILImage(),\n                transforms.ToTensor(),\n            ]\n        )\n\n    def is_tta(self):\n        return self._tta == True\n\n    def __len__(self):\n        return len(self._pixels)\n\n    def __getitem__(self, idx):\n        pixels = self._pixels[idx]\n        pixels = list(map(int, pixels.split(\" \")))\n        image = np.asarray(pixels).reshape(48, 48)\n        image = image.astype(np.uint8)\n\n        image = cv2.resize(image, self._image_size)\n        image = np.dstack([image] * 3)\n\n        if self._stage == \"train\":\n            image = seg(image=image)\n\n        if self._stage == \"test\" and self._tta == True:\n            images = [seg(image=image) for i in range(self._tta_size)]\n            # images = [image for i in range(self._tta_size)]\n            images = list(map(self._transform, images))\n            target = self._emotions.iloc[idx].idxmax()\n            return images, target\n\n        image = self._transform(image)\n        target = self._emotions.iloc[idx].idxmax()\n        return image, target\n\ndef fer2013(stage, configs=None, tta=False, tta_size=48):\n    return FER2013(stage, configs, tta, tta_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:13:05.667353Z","iopub.execute_input":"2024-06-10T15:13:05.667681Z","iopub.status.idle":"2024-06-10T15:13:05.687469Z","shell.execute_reply.started":"2024-06-10T15:13:05.667651Z","shell.execute_reply":"2024-06-10T15:13:05.686636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import datetime\nimport os\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\nEMO_DICT = {0: \"ne\", 1: \"an\", 2: \"di\", 3: \"fe\", 4: \"ha\", 5: \"sa\", 6: \"su\"}\n\ndef make_batch(images):\n    if not isinstance(images, list):\n        images = [images]\n    return torch.stack(images, 0)\n\ndef accuracy(output, target):\n    with torch.no_grad():\n        batch_size = target.size(0)\n        pred = torch.argmax(output, dim=1)\n        correct = pred.eq(target).float().sum(0)\n        acc = correct * 100 / batch_size\n    return [acc]\n\nclass RAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\"step\"] += 1\n                buffered = self.buffer[int(state[\"step\"] % 10)]\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = (\n                            group[\"lr\"]\n                            * math.sqrt(\n                                (1 - beta2_t)\n                                * (N_sma - 4)\n                                / (N_sma_max - 4)\n                                * (N_sma - 2)\n                                / N_sma\n                                * N_sma_max\n                                / (N_sma_max - 2)\n                            )\n                            / (1 - beta1 ** state[\"step\"])\n                        )\n                    else:\n                        step_size = group[\"lr\"] / (1 - beta1 ** state[\"step\"])\n                    buffered[2] = step_size\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass Trainer(object):\n    \"\"\"base class for trainers\"\"\"\n\n    def __init__(self):\n        pass\n\n\nclass FER2013Trainer(Trainer):\n    \"\"\"for classification task\"\"\"\n\n    def __init__(self, model, train_set, val_set, test_set, configs):\n        super().__init__()\n        print(\"Start trainer..\")\n        print(configs)\n\n        # load config\n        self._configs = configs\n        self._lr = self._configs[\"lr\"]\n        self._batch_size = self._configs[\"batch_size\"]\n        self._momentum = self._configs[\"momentum\"]\n        self._weight_decay = self._configs[\"weight_decay\"]\n        self._distributed = self._configs[\"distributed\"]\n        self._num_workers = self._configs[\"num_workers\"]\n        self._device = torch.device(self._configs[\"device\"])\n        self._max_epoch_num = self._configs[\"max_epoch_num\"]\n        self._max_plateau_count = self._configs[\"max_plateau_count\"]\n\n        # load dataloader and model\n        self._train_set = train_set\n        self._val_set = val_set\n        self._test_set = test_set\n        self._model = model(\n            in_channels= 3, # configs[\"in_channels\"],\n            num_classes= 7,# configs[\"num_classes\"],\n        )\n        self._model = self._model.to(self._device)\n\n        if self._distributed == 1:\n            torch.distributed.init_process_group(backend=\"nccl\")\n            self._model = nn.parallel.DistributedDataParallel(self._model)\n\n            self._train_loader = DataLoader(\n                self._train_set,\n                batch_size=self._batch_size,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=True,\n                worker_init_fn=lambda x: np.random.seed(x),\n            )\n            self._val_loader = DataLoader(\n                self._val_set,\n                batch_size=self._batch_size,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=False,\n                worker_init_fn=lambda x: np.random.seed(x),\n            )\n\n            self._test_loader = DataLoader(\n                self._test_set,\n                batch_size=1,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=False,\n                worker_init_fn=lambda x: np.random.seed(x),\n            )\n        else:\n            self._train_loader = DataLoader(\n                self._train_set,\n                batch_size=self._batch_size,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=True,\n            )\n            self._val_loader = DataLoader(\n                self._val_set,\n                batch_size=self._batch_size,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=False,\n            )\n            self._test_loader = DataLoader(\n                self._test_set,\n                batch_size=1,\n                num_workers=self._num_workers,\n                pin_memory=True,\n                shuffle=False,\n            )\n\n        # define loss function (criterion) and optimizer\n        class_weights = [\n            1.02660468,\n            9.40661861,\n            1.00104606,\n            0.56843877,\n            0.84912748,\n            1.29337298,\n            0.82603942,\n        ]\n        class_weights = torch.FloatTensor(np.array(class_weights))\n\n        if self._configs[\"weighted_loss\"] == 0:\n            self._criterion = nn.CrossEntropyLoss().to(self._device)\n        else:\n            self._criterion = nn.CrossEntropyLoss(class_weights).to(self._device)\n\n        self._optimizer = RAdam(\n            params=self._model.parameters(),\n            lr=self._lr,\n            weight_decay=self._weight_decay,\n        )\n\n        self._scheduler = ReduceLROnPlateau(\n            self._optimizer,\n            patience=self._configs[\"plateau_patience\"],\n            min_lr=1e-6,\n            verbose=True,\n        )\n\n        # training info\n        self._start_time = datetime.datetime.now()\n        self._start_time = self._start_time.replace(microsecond=0)\n\n        log_dir = os.path.join(\n            self._configs[\"cwd\"],\n            self._configs[\"log_dir\"],\n            \"{}_{}_{}\".format(\n                self._configs[\"arch\"],\n                self._configs[\"model_name\"],\n                self._start_time.strftime(\"%Y%b%d_%H.%M\"),\n            ),\n        )\n        self._writer = SummaryWriter(log_dir)\n        self._train_loss_list = []\n        self._train_acc_list = []\n        self._val_loss_list = []\n        self._val_acc_list = []\n        self._best_val_loss = 1e9\n        self._best_val_acc = 0\n        self._best_train_loss = 1e9\n        self._best_train_acc = 0\n        self._test_acc = 0.0\n        self._plateau_count = 0\n        self._current_epoch_num = 0\n\n        # for checkpoints\n        # really?\n        # self._checkpoint_dir = os.path.join(self._configs[\"cwd\"], \"saved/checkpoints\")\n        # if not os.path.exists(self._checkpoint_dir):\n        #     os.makedirs(self._checkpoint_dir, exist_ok=True)\n\n        self._checkpoint_dir = os.path.join(\n            self._configs[\"cwd\"], self._configs[\"checkpoint_dir\"]\n        )\n        if not os.path.exists(self._checkpoint_dir):\n            os.makedirs(self._checkpoint_dir, exist_ok=True)\n\n        self._checkpoint_path = os.path.join(\n            self._checkpoint_dir,\n            \"{}_{}_{}\".format(\n                self._configs[\"arch\"],\n                self._configs[\"model_name\"],\n                self._start_time.strftime(\"%Y%b%d_%H.%M\"),\n            ),\n        )\n\n    def _train(self):\n        self._model.train()\n        train_loss = 0.0\n        train_acc = 0.0\n\n        for i, (images, targets) in tqdm(\n            enumerate(self._train_loader), total=len(self._train_loader), leave=False\n        ):\n            images = images.cuda(non_blocking=True)\n            targets = targets.cuda(non_blocking=True)\n\n            # compute output, measure accuracy and record loss\n            outputs = self._model(images)\n\n            loss = self._criterion(outputs, targets)\n            acc = accuracy(outputs, targets)[0]\n            # acc = eval_metrics(targets, outputs, 2)[0]\n\n            train_loss += loss.item()\n            train_acc += acc.item()\n\n            # compute gradient and do SGD step\n            self._optimizer.zero_grad()\n            loss.backward()\n            self._optimizer.step()\n\n        i += 1\n        self._train_loss_list.append(train_loss / i)\n        self._train_acc_list.append(train_acc / i)\n\n    def _val(self):\n        self._model.eval()\n        val_loss = 0.0\n        val_acc = 0.0\n\n        with torch.no_grad():\n            for i, (images, targets) in tqdm(\n                enumerate(self._val_loader), total=len(self._val_loader), leave=False\n            ):\n                images = images.cuda(non_blocking=True)\n                targets = targets.cuda(non_blocking=True)\n\n                # compute output, measure accuracy and record loss\n                outputs = self._model(images)\n\n                loss = self._criterion(outputs, targets)\n                acc = accuracy(outputs, targets)[0]\n\n                val_loss += loss.item()\n                val_acc += acc.item()\n\n            i += 1\n            self._val_loss_list.append(val_loss / i)\n            self._val_acc_list.append(val_acc / i)\n\n    def _calc_acc_on_private_test(self):\n        self._model.eval()\n        test_acc = 0.0\n        print(\"Calc acc on private test..\")\n        f = open(\"private_test_log.txt\", \"w\")\n        with torch.no_grad():\n            for i, (images, targets) in tqdm(\n                enumerate(self._test_loader), total=len(self._test_loader), leave=False\n            ):\n\n                images = images.cuda(non_blocking=True)\n                targets = targets.cuda(non_blocking=True)\n\n                outputs = self._model(images)\n                print(outputs.shape, outputs)\n                acc = accuracy(outputs, targets)[0]\n                test_acc += acc.item()\n                f.writelines(\"{}_{}\\n\".format(i, acc.item()))\n\n            test_acc = test_acc / (i + 1)\n        print(\"Accuracy on private test: {:.3f}\".format(test_acc))\n        f.close()\n        return test_acc\n\n    def _calc_acc_on_private_test_with_tta(self):\n        self._model.eval()\n        test_acc = 0.0\n        print(\"Calc acc on private test with tta..\")\n        f = open(\n            \"private_test_log_{}_{}.txt\".format(\n                self._configs[\"arch\"], self._configs[\"model_name\"]\n            ),\n            \"w\",\n        )\n\n        with torch.no_grad():\n            for idx in tqdm(\n                range(len(self._test_set)), total=len(self._test_set), leave=False\n            ):\n                images, targets = self._test_set[idx]\n                targets = torch.LongTensor([targets])\n\n                images = make_batch(images)\n                images = images.cuda(non_blocking=True)\n                targets = targets.cuda(non_blocking=True)\n\n                outputs = self._model(images)\n                outputs = F.softmax(outputs, 1)\n\n                # outputs.shape [tta_size, 7]\n                outputs = torch.sum(outputs, 0)\n\n                outputs = torch.unsqueeze(outputs, 0)\n                # print(outputs.shape)\n                # TODO: try with softmax first and see the change\n                acc = accuracy(outputs, targets)[0]\n                test_acc += acc.item()\n                f.writelines(\"{}_{}\\n\".format(idx, acc.item()))\n\n            test_acc = test_acc / (idx + 1)\n        print(\"Accuracy on private test with tta: {:.3f}\".format(test_acc))\n        f.close()\n        return test_acc\n\n    def train(self):\n        \"\"\"make a training job\"\"\"\n        try:\n            while not self._is_stop():\n                self._increase_epoch_num()\n                self._train()\n                self._val()\n\n                self._update_training_state()\n                self._logging()\n        except KeyboardInterrupt:\n            traceback.print_exc()\n\n        # training stop\n        try:\n            # state = torch.load('saved/checkpoints/resatt18_rot30_2019Nov06_18.56')\n            state = torch.load(self._checkpoint_path)\n            if self._distributed:\n                self._model.module.load_state_dict(state[\"net\"])\n            else:\n                self._model.load_state_dict(state[\"net\"])\n\n            if not self._test_set.is_tta():\n                self._test_acc = self._calc_acc_on_private_test()\n            else:\n                self._test_acc = self._calc_acc_on_private_test_with_tta()\n\n            # self._test_acc = self._calc_acc_on_private_test()\n            self._save_weights()\n        except Exception:\n            traceback.print_exc()\n\n        consume_time = str(datetime.datetime.now() - self._start_time)\n        self._writer.add_text(\n            \"Summary\",\n            \"Converged after {} epochs, consume {}\".format(\n                self._current_epoch_num, consume_time[:-7]\n            ),\n        )\n        self._writer.add_text(\n            \"Results\", \"Best validation accuracy: {:.3f}\".format(self._best_val_acc)\n        )\n        self._writer.add_text(\n            \"Results\", \"Best training accuracy: {:.3f}\".format(self._best_train_acc)\n        )\n        self._writer.add_text(\n            \"Results\", \"Private test accuracy: {:.3f}\".format(self._test_acc)\n        )\n        self._writer.close()\n\n    def _update_training_state(self):\n        if self._val_acc_list[-1] > self._best_val_acc:\n            self._save_weights()\n            self._plateau_count = 0\n            self._best_val_acc = self._val_acc_list[-1]\n            self._best_val_loss = self._val_loss_list[-1]\n            self._best_train_acc = self._train_acc_list[-1]\n            self._best_train_loss = self._train_loss_list[-1]\n        else:\n            self._plateau_count += 1\n\n        # self._scheduler.step(self._train_loss_list[-1])\n        self._scheduler.step(100 - self._val_acc_list[-1])\n        # self._scheduler.step()\n\n    def _logging(self):\n        consume_time = str(datetime.datetime.now() - self._start_time)\n\n        message = \"\\nE{:03d}  {:.3f}/{:.3f}/{:.3f} {:.3f}/{:.3f}/{:.3f} | p{:02d}  Time {}\\n\".format(\n            self._current_epoch_num,\n            self._train_loss_list[-1],\n            self._val_loss_list[-1],\n            self._best_val_loss,\n            self._train_acc_list[-1],\n            self._val_acc_list[-1],\n            self._best_val_acc,\n            self._plateau_count,\n            consume_time[:-7],\n        )\n\n        self._writer.add_scalar(\n            \"Accuracy/Train\", self._train_acc_list[-1], self._current_epoch_num\n        )\n        self._writer.add_scalar(\n            \"Accuracy/Val\", self._val_acc_list[-1], self._current_epoch_num\n        )\n        self._writer.add_scalar(\n            \"Loss/Train\", self._train_loss_list[-1], self._current_epoch_num\n        )\n        self._writer.add_scalar(\n            \"Loss/Val\", self._val_loss_list[-1], self._current_epoch_num\n        )\n\n        print(message)\n\n    def _is_stop(self):\n        \"\"\"check stop condition\"\"\"\n        return (\n            self._plateau_count > self._max_plateau_count\n            or self._current_epoch_num > self._max_epoch_num\n        )\n\n    def _increase_epoch_num(self):\n        self._current_epoch_num += 1\n\n    def _save_weights(self, test_acc=0.0):\n        if self._distributed == 0:\n            state_dict = self._model.state_dict()\n        else:\n            state_dict = self._model.module.state_dict()\n\n        state = {\n            **self._configs,\n            \"net\": state_dict,\n            \"best_val_loss\": self._best_val_loss,\n            \"best_val_acc\": self._best_val_acc,\n            \"best_train_loss\": self._best_train_loss,\n            \"best_train_acc\": self._best_train_acc,\n            \"train_losses\": self._train_loss_list,\n            \"val_loss_list\": self._val_loss_list,\n            \"train_acc_list\": self._train_acc_list,\n            \"val_acc_list\": self._val_acc_list,\n            \"test_acc\": self._test_acc,\n        }\n\n        torch.save(state, self._checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:13:05.688963Z","iopub.execute_input":"2024-06-10T15:13:05.689297Z","iopub.status.idle":"2024-06-10T15:13:20.976866Z","shell.execute_reply.started":"2024-06-10T15:13:05.689269Z","shell.execute_reply":"2024-06-10T15:13:20.976076Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-06-10 15:13:09.470304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-10 15:13:09.470449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-10 15:13:09.746640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport os\nimport random\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\nimport imgaug\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\n\nseed = 1234\nrandom.seed(seed)\nimgaug.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\ndef main(config_path):\n    \"\"\"\n    This is the main function to make the training up\n\n    Parameters:\n    -----------\n    config_path : srt\n        path to config file\n    \"\"\"\n    # load configs and set random seed\n    configs = json.load(open(config_path))\n    configs[\"cwd\"] = os.getcwd()\n\n    # load model and data_loader\n    model = get_model(configs)\n\n    train_set, val_set, test_set = get_dataset(configs)\n\n    # init trainer and make a training\n    # from trainers.fer2013_trainer import FER2013Trainer\n\n    # from trainers.centerloss_trainer import FER2013Trainer\n    trainer = FER2013Trainer(model, train_set, val_set, test_set, configs)\n\n    if configs[\"distributed\"] == 1:\n        ngpus = torch.cuda.device_count()\n        mp.spawn(trainer.train, nprocs=ngpus, args=())\n    else:\n        trainer.train()\n\ndef get_model(configs):\n    \"\"\"\n    This function get raw models from models package\n\n    Parameters:\n    ------------\n    configs : dict\n        configs dictionary\n    \"\"\"\n    try:\n        return resmasking_dropout1                   # models.__dict__[configs[\"arch\"]]\n    except:\n      print(\"Cant load the model\")\n\ndef get_dataset(configs):\n    \"\"\"\n    This function get raw dataset\n    \"\"\"\n    # todo: add transform\n    train_set = fer2013(\"train\", configs)\n    val_set = fer2013(\"val\", configs)\n    test_set = fer2013(\"test\", configs, tta=True, tta_size=10)\n    return train_set, val_set, test_set\n\n\nif __name__ == \"__main__\":\n    main(\"/kaggle/input/configs/fer2013_configs.json\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T12:28:28.826301Z","iopub.execute_input":"2024-05-06T12:28:28.827415Z","iopub.status.idle":"2024-05-06T12:28:41.944593Z","shell.execute_reply.started":"2024-05-06T12:28:28.827380Z","shell.execute_reply":"2024-05-06T12:28:41.943111Z"},"trusted":true},"execution_count":null,"outputs":[]}]}